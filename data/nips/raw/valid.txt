0	Area under ROC AUC is a metric which is widely used for measuring the classification performance for imbalanced data. It is of theoretical and practical interest to develop online learning algorithms that maximizes AUC for large-scale data. A specific challenge in developing online AUC maximization algorithm is that the learning objective function is usually defined over a pair of training examples of opposite classes, and existing methods achieves on-line processing with higher space and time complexity. In this work, we propose a new stochastic online algorithm for AUC maximization. In particular, we show that AUC optimization can be equivalently formulated as a convex-concave saddle point problem. From this saddle representation, a stochastic online algorithm SOLAM is proposed which has time and space complexity of one datum. We establish theoretical convergence of SOLAM with high probability and demonstrate its effectiveness on standard benchmark datasets. Introduction Area Under the ROC Curve AUC is a widely used metric for measuring classification performance. Unlike misclassification error that reflects a classifiers ability to classify a single randomly chosen example, AUC concerns the overall performance of a functional family of classifiers and quantifies their ability of correctly ranking any positive instance with regards to a randomly chosen negative instance. Most algorithms optimizing AUC for classification , , , are for batch learning, where we assume all training data are available. On the other hand, online learning algorithms , , , , , , have been proven to be very efficient to deal with large-scale datasets. However, most studies of online learning focus on the misclassification error or its surrogate loss, in which the objective function depends on a sum of losses over individual examples. It is thus desirable to develop online learning algorithms to optimize the AUC metric. The main challenge for an online AUC algorithm is that the objective function of AUC maximization depends on a sum of pairwise losses between instances from different classes which is quadratic in the number of training examples. As such, directly deploying the existing online algorithms will require to store all training data received, making it not feasible for large-scale data analysis. Several recent works , , , , have studied a type of online AUC maximization method that updates the classifier upon the arrival of each new training example. However, this type of algorithms need to access all previous examples at iteration t, and has Otd space and per-iteration complexity where d is the dimension of the data. The scaling of per-iteration space and time complexity is an undesirable property for online applications that have to use fixed resources. This problem is partially alleviated by the use of buffers of a fixed size s in , , which reduces the per-iteration space and time complexity to Osd. Although this change makes the per-iteration space and time complexity independent of the number of iterations, in practice, to reduce variance in learning performance, the th Conference on Neural Information Processing Systems NIPS , Barcelona, Spain. size of the buffer needs to be set sufficiently large. The work of proposes an alternative method that requires to update and store the first-order mean and second-order covariance statistics of the training data, and the space and per-iteration complexity becomes Od . Although this eliminates the needs to access all previous training examples, the per-iteration is now quadratic in data dimension, which makes this method inefficient for high-dimensional data. To this end, the authors of further proposed to approximate the covariance matrices with low-rank random Gaussian matrices. However, the approximation method is not a general solution to the original problem and its convergence was only established under the assumption that the effective numerical rank for the set of covariance matrices is small i.e., they can be well approximated by low-rank matrices. In this work, we present a new stochastic online AUC maximization SOLAM method associated for the loss function. In contrast to existing online AUC maximization methods, e.g. , , SOLAM does not need to store previously received training examples or the covariance matrices, while, at the same time, enjoys a comparable convergence rate, up to a logarithmic term, as in , . To our best knowledge, this is the first online learning algorithm for AUC optimization with linear space and per-iteration time complexities of Od, which are the same as the online gradient descent algorithm , , , for classification. The key step of SOLAM is to reformulate the original problem as a stochastic saddle point problem . This connection is the foundation of the SOLAM algorithm and its convergence analysis. When evaluating on several standard benchmark datasets, SOLAM achieves performances that are on par with state-of-the-art online AUC optimization methods with significant improvement in running time. The main contribution of our work can be summarized as follows We provide a new formulation of the AUC optimization problem as stochastic Saddle Point Problem SPP. This formulation facilitates the development of online algorithms for AUC optimization. Our algorithm SOLAM achieves a per-iteration space and time complexity that is linear in data dimensionality. Our theoretical analysis provides guarantee of convergence, with high probability, of the proposed algorithm. Method Let the input space X Rd and the output space Y , . We assume the training data, z xi , yi , i , . . . , n as i.i.d. sample drawn from an unknown distribution on Z X Y. The ROC curve is the plot of the true positive rate versus the false positive rate. The area under the ROC curve AUC for any scoring function f X R is equivalent to the probability of a positive sample ranks higher than a negative sample e.g. , . It is defined as AUCf Prf x f x y , y , where x, y and x , y are independent drawn from . The target of AUC maximization is to find the optimal decision function f arg max AUCf arg min Prf x f x y , y f f h i arg min E If x f x y , y , f where I is the indicator function that takes value if the argument is true and otherwise. Let p Pry . For any RR random variable z, recall that its conditional expectation is defined by Ezy p zIy dz. Since I is not continuous, it is often replaced by its convex surrogates. Two common choices are the loss f x f x or the hinge loss f x f x . In this work, we use the , as it has been shown to be statistically consistent with AUC while the hinge loss is not , . We also restrict our interests to the family of linear functions, i.e., f x w x. In summary, the AUC maximization can be formulated by h i argminkwkR E w x x y , y RR argminkwkR pp w x x Iy Iy dzdz . ZZ When is a uniform distribution over training data z, we obtain the empirical minimization ERM problem for AUC optimization studied in , argmin kwkR n n XX w xi xj Iyi yj , n n i j where n and n denote the numbers of instances in the positive and negative classes, respectively. . Equivalent Representation as a Stochastic Saddle Point Problem SPP The main result of this work is the equivalence of problem to a stochastic Saddle Point Problem SPP e.g., . A stochastic SPP is generally in the form of min max f u, EF u, , , u d m where R and R are nonempty closed convex sets, is a random vector with non-empty R measurable set Rp , and F R. Here EF u, , F u, , d Pr, and function f u, is convex in u and concave in . In general, u and are referred to as the primal variable and the dual variable, respectively. The following theorem shows that is equivalent to a stochastic SPP . First, define F Rd R Z R, for any w Rd , a, b, R and z x, y Z, by F w, a, b, z pw x a Iy pw x b Iy pw xIy pw xIy p p . Theorem . The AUC optimization is equivalent to Z n o min max f w, a, b, F w, a, b, zdz . kwkR a,bR R Z Proof. It suffices toR prove the claim that the objective function of equals to mina,bR maxR Z F w, a, b, zdz. To this end, note that z x, y and z x , y are samples independently drawn from . Therefore, the objective function of can be rewritten as E w x x y , y Ew x y Ew x y Ew xy Ew x y Ew xy Ew x y Ew x y Ew xy Ew x y Ew x y Ew xy Ew x y Ew xy Ew x y . R Note that Ew x y Ew xy p Z w x Iy dz R R minaR p Z w xa Iy dz minaR Ew xa y , p Z w xIy dz where the minimization is achieved by a Ew xy . Likewise, min Ew x b y Ew x y Ew x y b where the minimization is obtained by letting b Ew x y . Moreover, observe that Ew xy Ew x y max Ew x y Ew xy , where the maximization is achieved with Ew x y Ew xy . minwRd nn Pn Pn The work , studied the regularized ERM problem, i.e. i j w xi xj Iyi Iyj kwk , which is equivalent to with being a bounded ball in Rd . Putting all these equalities into implies that R h i F w, a, b zdz E w x x y , y min max Z . p p a,bR R This proves the claim and hence the theorem. In addition, we can prove the following result. Proposition . Function f w, a, b, is convex in w, a, b Rd and concave in R. The proof of this proposition can be found in the Supplementary Materials. . Stochastic Online Algorithm for AUC Maximization The optimal solution to an SPP problem is called a saddle point. Stochastic first-order methods are widely used to get such an optimal saddle point. The main idea of such algorithms e.g. , is to use an unbiased stochastic estimator of the true gradient to perform, at each iteration, gradient descent in the primal variable and gradient ascent in the dual variable. Using the stochastic SPP formulation for AUC optimization, we can develop stochastic online learning algorithms which only need to pass the data once. For notational simplicity, let vector v w , a, b Rd , and for any w Rd , a, b, R and z x, y Z, we denote f w, a, b, as f v, , and F w, a, b, , z as F v, , z. The gradient of the objective function in the stochastic SPP problem is given by a d -dimensional column vector gv, v f v, , f v, and its unbiased stochastic estimator is given, for any z Z, by Gv, , z u F v, , z, F v, , z. One could directly deploy the stochastic first-order method in to the stochastic SPP formulation for AUC optimization. However, from the definition of F in , this would require the knowledge of the unknown probability p Pry a priori. To overcome this problem, for any v w , a, b Rd , R and z Z, let Ft v, , z pt w x a Iy pt w x b Iy pt w xIy pt w xIy pt pt . Pt where pt i Iyi t at iteration t. We propose, at iteration t, to use the stochastic estimator t v, , z v Ft v, , z, Ft v, , z G to replace the unbiased, but practically inaccessible, stochastic estimator Gv, , z. Assume supxX kxk , and recall that kwk R. For any optimal solution wR, a , b of the stochastic SPP for AUC optimization, by , and we know that a p Z hw , xiIy dz R R R, b p Z hw , x iIy dz R, and p hw , x iIy dz Z R p Z hw , xiIy dz R. Therefore, we can restrict w, a, b and to the following bounded domains w, a, b Rd kwk R, a R, b R , R R . In this case, the projection steps e.g. steps and in Table can be easily computed. The pseudocode of the online AUC optimization algorithm is described in Table , to which we refer as SOLAM. Analysis We now present the convergence results of the proposed algorithm for AUC optimization. Let u v, w, a, b, . The quality of an approximation solution vt , t to the SPP problem at iteration t is measured by the duality gap f vt , t max f vt , min f v, t . v Stochastic Online AUC Maximization SOLAM . Choose step sizes t t N . Initialize t , v , and let p , v , and . tpt Iyt . Receive a sample zt xt , yt and compute pt t . Update vt P vt t v Ft vt , t , zt . Update t P t t Ft vt , t , zt . Update t t t t vt t vt , and t t t t t t . Update vt t . Set t t Table Pseudo code of the proposed algorithm. In steps and , P and P denote the projection to the convex sets and , respectively. Theorem . Assume that samples x , y , x , y , . . . , xT , yT are i.i.d. drawn from a distribution over X Y, let and be given by and the step sizes given by t t N. For sequence vt , t t , T generated by SOLAM Table , and any , with probability , the following holds r T T T T X X X T X h i j , j j j f vT , T C maxR , ln j j j j j where C is an absolute constant independent of R and T see its explicit expression in the proof. Denote f as the optimum of which, by Theorem , is identical to the optimal value of AUC optimization . From Theorem , the following convergence rate is straightforward. Corollary . Under the same assumptions as in Theorem , and j j j N with constant r ln T ln T , with probability , it holds f vT , T f f uT O . T While the above convergence rate is obtained by choosing decaying step sizes, one can establish a similar result when a constant step size is appropriately chosen. The proof of Theorem requires several lemmas. The first is a standard result from convex online learning , . We include its proof in the Supplementary Materials for completeness. Lemma . For any T N, let j j , T be a sequence of vectors in Rm , and u where is a convex set. For any t , T define u t P ut t . Then, for any u , there holds PT PT ut u t ku uk t kt k . t The second lemma is the Pinelis-Bernstein inequality for martingale difference sequence in a Hilbert space, which is from , Theorem . Lemma . Let Sk k N be a martingale difference sequence in a Hilbert space. Suppose that PT almost surely kSk k B and k EkSk k S , . . . , Sk T . Then, for any , there Pj holds, with probability at least , supjT k Sk B T log . j u, z defined by , is not The third lemma indicates that the approximate stochastic estimator G far away from the unbiased one Gu, z. Its proof is given in the Supplementary materials. Lemma . Let and be given by and denote by . For any t N, with t u, z Gu, zk R R ln t . probability , there holds sup kG u,zZ Proof of Theorem . By the convexity of f , and concavity of of f v, , for any u v, , we get f vt , f v, t f vt , t f v, t f vt , f vt , t vt v v f vt , t t f vt , t ut u gut . Hence, there holds T T T X X X max f vT , min f v, T t max t f vt , min t f v, t v t t v t T X t t max u T X t ut u gut t Recall that . The steps and in Algorithm SOLAM can be rewritten as ut t ut , zt . By applying Lemma with t t G t ut , zt , we have, for vt , t P ut t G PT PT ku uk any u , that t t ut u Gt ut , zt t t kGt ut , zt k , which yields that T T X ku uk X sup t ut u gut sup kGt ut , zt k t t u t u T T X X t ut , zt k t ut , zt sup ku uk kG t ut u gut G t t u t u sup sup T X t ut u gut Gut , zt sup u t T X t ut , zt t ut u Gut , zt G u t Now we estimate the terms on the right hand side of as follows. For the first term, we have sup ku uk sup kvk sup kuk R . u v , u For the second term on the right hand side of , observe that supxX kxk and ut wt , at , bt , t w, a, b, kwk R, a R, b R, R . Combining this t ut , zt given by , one can easily get kG t ut , zt k kw Ft ut , zt k with the definition of G a Ft ut , zt b Ft ut , zt Ft ut , zt R R. Hence, there holds T T X X t . t kGt ut , zt k R R t t PT The third term on the right hand side of can be bounded by supu t t ut u gut PT PT Gut , zt supu t t ut u gut Gut , zt t t ut u t gut Gut , zt , where u and u t P ut t gut Gut , zt for any t , T . Applying Lemma with t t gut Gut , zt yields that sup T X T kuk X kgut Gut , zt k t t u t ut u gut Gut , zt sup u t T X R R R t , t where we used kGut , zt k and kgut k is uniformly bounded by R R. Notice that ut and u t are only dependent on z , z , . . . , zt , St t ut u t gut Gut , zt t a martingale difference sequence. Observe that EkSt k z , . . . , zt RR , . . . , t is t Z ut u t gut Gut , z dz t supu,zZ kut u t k kgut Gut , zt k t R R R . Applying Lemma with T R R PT R t t , B supTt t ut u t gut Gut , zt T implies that, with probabili ty , there holds v u T T X X R R R u t t ut u t gut Gut , zt t . t t Combining with implies, with probability , T X T X R sup t ut u gut Gut , zt R R t u t t datasets inst feat datasets inst feat datasets inst feat datasets inst feat diabetes fourclass german , splice , usps , aa , mnist , acoustic , ijcnn , covtype , sector , , news , , Table Basic information about the benchmark datasets used in the experiments. T R R R X . t t t u, zGu, zk By Lemma , for any t , T there holds, with probability T , sup kG u,zZ r T R ln t. Hence, the fourth term on the righthand side of can estimated as follows with probability , there holds sup T X t ut , zt sup kuk t ut u Gut , zt G u t u T X t t t u, z Gu, zk kG sup u,zZ RR R p T X t . t t Putting the estimations , , , and back into implies that r T T T T X X X T X h i t , t t t f uT C maxR , ln t t t t t where C . Experiments In this section, we report experimental evaluations of the SOLAM algorithm and comparing its performance with existing state-of-the-art learning algorithms for AUC optimization. SOLAM was implemented in MATLAB, and MATLAB code of the compared methods were obtained from the authors of corresponding papers. In the training phase, we use five-fold cross validation to determine the initial learning rate and the bound on w, R by a grid search. Following the evaluation protocol of , the performance of SOLAM was evaluated by averaging results from five runs of five-fold cross validations. Our experiments were performed based on datasets that had been used in previous studies. For multi-class datasets, e.g., news and sector, we transform them into binary classification problems by randomly partitioning the data into two groups, where each group includes the same number of classes. Information about these datasets is summarized in Table . On these datasets, we evaluate and compare SOLAM with four online and two offline learning algorithms for AUC maximization, i.e. one-pass AUC maximization OPAUC , which uses the loss surrogate of the AUC objective function online AUC maximization that uses the hinge loss surrogate of the AUC objective function with two variants, one with sequential update OAMseq and the other using gradient update OAMgra online Uni-Exp which uses the weighted univariate exponential loss B-SVM-OR , which is a batch learning algorithm using the hinge loss surrogate of the AUC objective function and B-LS-SVM, which is a batch learning algorithm using the loss surrogate of the AUC objective function. Classification performances on the testing dataset of all methods are given in Table . These results show that SOLAM achieves similar performances as other state-of-the-art online and offline methods based on AUC maximization. The performance of SOLAM is better than the offline methods on acoustic and covtype which could be due to the normalization of features used in our experiments for SOLAM. On the other hand, the main advantage of SOLAM is the running efficiency, as we pointed out in the Introduction, its per-iteration running time and space complexity is linear in data dimension and do not depend on the iteration number. In Figure , we show AUC vs. run time seconds for Datasets diabetes fourclass german splice usps aa mnist acoustic ijcnn covtype sector news SOLAM OPAUC OAMseq OAMgra online Uni-Exp B-SVM-OR B-LS-SVM .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. .. Table Comparison of the testing AUC values meanstd. on the evaluated datasets. To accelerate the experiments, the performances of OPAUC, OAMseq, OAMgra, online Uni-Exp, B-SVM-OR and B-LS-SVM were taken from a aa b ups c sector Figure AUC vs. time curves of SOLAM algorithm and three state-of-the-art AUC learning algorithms, i.e., OPAUC , OAMseq , and OAMgra . The values in parentheses indicate the average running time seconds per pass for each algorithm. SOLAM and three other state-of-the-art online learning algorithms,i.e., OPAUC , OAMseq , and OAMgra over three datasets aa, ups, and sector, along with the per-iteration running time in the legend . These results show that SOLAM in general reaches convergence faster in comparison of, while achieving competitive performance. Conclusion In this paper we showed that AUC maximization is equivalent to a stochastic saddle point problem, from which we proposed a novel online learning algorithm for AUC optimization. In contrast to the existing algorithms , , the main advantage of our algorithm is that it does not need to store all previous examples nor its second-order covariance matrix. Hence, it is a truly online learning algorithm with one-datum space and per-iteration complexities, which are the same as online gradient descent algorithms for classification. There are several research directions for future work. Firstly, the convergence rate O T for SOLAM only matches that of the black-box sub-gradient method. It would be interesting to derive fast convergence rate OT by exploring the special structure of the objective function F defined by . Secondly, the convergence was established using the duality gap associated with the stochastic SPP formulation . It would be interesting to establish the strong convergence of the output w T of algorithm SOLAM to its optimal solution of the actual AUC optimization problem . Thirdly, the SPP formulation holds for the least square loss. We do not know if the same formulation holds true for other loss functions such as the logistic regression or the hinge loss. Experiments were performed with running time reported based on a workstation with nodes, each with an Intel Xeon E- .GHz CPU and GB RAM. 
0	We propose a goal-driven web navigation as a benchmark task for evaluating an agent with abilities to understand natural language and plan on partially observed environments. In this challenging task, an agent navigates through a website, which is represented as a graph consisting of web pages as nodes and hyperlinks as directed edges, to find a web page in which a query appears. The agent is required to have sophisticated high-level reasoning based on natural languages and efficient sequential decision-making capability to succeed. We release a software tool, called WebNav, that automatically transforms a website into this goal-driven web navigation task, and as an example, we make WikiNav, a dataset constructed from the English Wikipedia. We extensively evaluate different variants of neural net based artificial agents on WikiNav and observe that the proposed goal-driven web navigation well reflects the advances in models, making it a suitable benchmark for evaluating future progress. Furthermore, we extend the WikiNav with questionanswer pairs from Jeopardy and test the proposed agent based on recurrent neural networks against strong inverted index based search engines. The artificial agents trained on WikiNav outperforms the engined based approaches, demonstrating the capability of the proposed goal-driven navigation as a good proxy for measuring the progress in real-world tasks such as focused crawling and question-answering. Introduction In recent years, there have been many exciting advances in building an artificial agent, which can be trained with one learning algorithm, to solve many relatively large-scale, complicated tasks see, e.g., , , . In much of these works, target tasks were computer games such as Atari games and racing car game . These successes have stimulated researchers to apply a similar learning mechanism to language-based tasks, such as multi-user dungeon MUD games , . Instead of visual perception, an agent perceives the state of the world by its written description. A set of actions allowed to the agent is either fixed or dependent on the current state. This type of task can efficiently evaluate the agents ability of not only in planning but also language understanding. We, however, notice that these MUD games do not exhibit the complex nature of natural languages to the full extent. For instance, the largest game world tested by Narasimhan et al. uses a vocabulary of only unique words, and the largest game tested by He et al. uses only words. Furthermore, the description of a state at each time step is almost always limited to the visual description of the current scene, lacking any use of higher-level concepts present in natural languages. In this paper, we propose a goal-driven web navigation as a large-scale alternative to the text-based games for evaluating artificial agents with natural language understanding and planning capability. The proposed goal-driven web navigation consists of the whole website as a graph, in which the web pages are nodes and hyperlinks are directed edges. An agent is given a query, which consists of one th Conference on Neural Information Processing Systems NIPS , Barcelona, Spain. or more sentences taken from a randomly selected web page in the graph, and navigates the network, starting from a predefined starting node, to find a target node in which the query appears. Unlike the text-based games, this task utilizes the existing text as it is, resulting in a large vocabulary with a truly natural language description of the state. Furthermore, the task is more challenging as the action space greatly changes with respect to the state in which the agent is. We release a software tool, called WebNav, that converts a given website into a goal-driven web navigation task. As an example of its use, we provide WikiNav, which was built from English Wikipedia. We design artificial agents based on neural networks called NeuAgents trained with supervised learning, and report their respective performances on the benchmark task as well as the performance of human volunteers. We observe that the difficulty of a task generated by WebNav is well controlled by two control parameters the maximum number of hops from a starting to a target node Nh and the length of query Nq . Furthermore, we extend the WikiNav with an additional set of queries that are constructed from Jeopardy questions, to which we refer by WikiNav-Jeopardy. We evaluate the proposed NeuAgents against the three search-based strategies SimpleSearch, Apache Lucene and Google Search API. The result in terms of document recall indicates that the NeuAgents outperform those search-based strategies, implying a potential for the proposed task as a good proxy for practical applications such as question-answering and focused crawling. Goal-driven Web Navigation A task T of goal-driven web navigation is characterized by T A, sS , G, q, R, . The world in which an agent A navigates is represented as a graph G N , E. The graph consists NN of a set of nodes N si i and a set of directed edges E ei,j connecting those nodes. Each node represents a page of the website, which, in turn, is represented by the natural language text Dsi in it. There exists an edge going from a page si to sj if and only if there is a hyperlink in Dsi that points to sj . One of the nodes is designated as a starting node sS from which any navigation begins. A target node is the one whose natural language description contains a query q, and there may be more than one target node. At each time step, the agent A reads the natural language description Dst of the current node in which the agent has landed. At no point, the whole world, consisting of the nodes and edges, nor its structure or map graph structure without any natural language description is visible to the agent, thus making this task partially observed. Once the agent A reads the description Dsi of the current node si , it can take one of the actions available. A set of possible actions is defined as a union of all the outgoing edges ei, and the stop action, thus making the agent have state-dependent action space. Each edge ei,k corresponds to the agent jumping to a next node sk , while the stop action corresponds to the agent declaring that the current node si is one of the target nodes. Each edge ei,k is represented by the description of the next node Dsk . In other words, deciding which action to take is equivalent to taking a peek at each neighboring node and seeing whether that node is likely to lead ultimately to a target node. The agent A receives a reward Rsi , q when it chooses the stop action. This task uses a simple binary reward, where , if q Dsi Rsi , q , otherwise Constraints It is clear that there exists an ultimate policy for the agent to succeed at every trial, which is to traverse the graph breadth-first until the agent finds a node in which the query appears. To avoid this kind of degenerate policies, the task includes a set of four rulesconstraints . An agent can follow at most Nn edges at each node. . An agent has a finite memory of size smaller than T . Table Dataset Statistics of WikiNav--, WikiNav--, WikiNav-- and WikiNav-Jeopardy. Train WikiNav-- .k WikiNav-- M WikiNav-- M WikiNav-Jeopardy k Valid k k k k Test k k k k . An agent moves up to Nh hops away from sS . . A query of size Nq comes from at least two hops away from the starting node. The first constraint alone prevents degenerate policies, such as breadth-first search, forcing the agent to make good decisions as possible at each node. The second one further constraints ensure that the agent does not cheat by using earlier trials to reconstruct the whole graph structure during test time or to store the entire world in its memory during training. The third constraint, which is optional, is there for computational consideration. The fourth constraint is included because the agent is allowed to read the content of a next node. WebNav Software As a part of this work, we build and release a software tool which turns a website into a goal-driven web navigation task. We call this tool WebNav. Given a starting URL, the WebNav reads the whole website, constructs a graph with the web pages in the website as nodes. Each node is assigned a unique identifier si . The text content of each node Dsi is a cleaned version of the actual HTML content of the corresponding web page. The WebNav turns intra-site hyperlinks into a set of edges ei,j . In addition to transforming a website into a graph G from Eq. , the WebNav automatically selects queries from the nodes texts and divides them into training, validation, and test sets. We ensure that there is no overlap among three sets by making each target node, from which a query is selected, belongs to only one of them. Each generated example is defined as a tuple X q, s , p where q is a query from a web page s , which was found following a randomly selected path p sS , . . . , s . In other words, the WebNav starts from a starting page sS , random-walks the graph for a predefined number of steps Nh , in our case, reaches a target node s and selects a query q from Ds . A query consists of Nq sentences and is selected among the top- candidates in the target node with the highest average TF-IDF, thus discouraging the WebNav from choosing a trivial query. For the evaluation purpose alone, it is enough to use only a query q itself as an example. However, we include both one target node among potentially many other target nodes and one path from the starting node to this target node again, among many possible connecting paths so that they can be exploited when training an agent. They are not to be used when evaluating a trained agent. WikiNav A Benchmark Task With the WebNav, we built a benchmark goal-driven navigation task using Wikipedia as a target website. We used the dump file of the English Wikipedia from September , which consists of more than five million web pages. We built a set of separate tasks with different levels of difficulty by varying the maximum number of allowed hops Nh , , and the size of query Nq , , . We refer to each task by WikiNav-Nh -Nq . For each task, we generate training, validation and test examples from the pages half as many hops away from a starting page as the maximum number of hops allowed. We use CategoryMain topic classifications as a starting node sS . The source code and datasets are publicly available at github.comnyu-dlWebNav. This limit is an artificial limit we chose for computational reasons. Table Sample query-answer pairs from WikiNav-Jeopardy. Query Answer For the last years of his life, Galileo was under house arrest for espousing this mans theory. Copernicus In the winter of -, a record , inches of snow fell at Rainier Paradise Ranger Station in this state. Washington This companys Accutron watch, introduced in , had a guarantee of accuracy to within one minute a month. Bulova As a minimal cleanup procedure, we excluded meta articles whose titles start with Wikipedia. Any hyperlink that leads to a web page outside Wikipedia is removed in advance together with the following sections 